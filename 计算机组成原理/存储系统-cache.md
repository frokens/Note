### 3.6.1 cache 基本原理
#### 1. cache 的功能
cache 是一种高速缓冲存储器，是为了解决 CPU 和主存之间速度不匹配而采用的一项重要技术。
通常由 [[存储系统-概述#3.2 静态随机存取存储器|SRAM]]实现（速度快，但容量小），充分利用程序的局部性原理，将最近用到的数据和相邻数据(时间和空间)，保存在cache中。
![[存储系统-概述#1. 程序的局部性原理]]

![[Pasted image 20250325093149.png]]

#### 2. cache 的基本原理
cache的控制逻辑：
- 在cpu外：与主存控制逻辑合成在一起(主存/chace 控制器)
- 在cpu内：由cpu控制

数据交换：
- cpu与cache间： 以字为单位
- 主存与cache间：以块为单位，一块由若干字组成
![[Pasted image 20250325093553.png]]

流程：
cpu 发送地址到 内存和cache
若cache的控制逻辑判断在cache内（cache命中）
	则直接传送给cpu
若cache缺失（未命中）
	则用主存读周期把此字从主存读出送到 CPU
	且把包括这个字的整个数据块传到cache

cache命中率：
$$h=\frac{N_c}{N_c+N_m}$$
设 $N_c$表示 cache 完成存取的总次数，$N_m$表示主存完成存取的总次数

cache/主存系统的平均访问时间 $t_a$:
	$$t_a= ht_c + (1-h)t_m$$
若 $t_c$表示命中时的 cache 访问时间，$t_m$表示未命中时的主存访问时间

目标为平均时间$t_a$ 接近$t_c$ ,则定义访问效率e:
$$e=\frac{t_c}{t_a}= \frac{t_c}{ht_c + (1-h)t_m} = \frac{1}{h + (1-h)r} = \frac{1}{r+(1-r)h}$$
其中$r =t_m/t_C$ ，所以命中率 h 越接近 1 越好。r 值以 5～10 为宜，不宜太大。


### 3.6.2 主存与 cache 的地址映射（空间局部性）
解决判断cache内是否存在数据 和 数据块 在内存和cache间的映射关系

数据结构：
cache内数据块称为行$L_i$，共m个，主存内数据块称其为块$B_j$，共n块，行内和块内容量大小一致。
一块或行内由k个连续的字组成，字是 CPU 每次访问存储器时可存取的最小单位。

#### 1. 全相联映射方式（M: N）
cache 一行由 标签(tag) + 内容 组成
其中内存的地址存在标签中，内容(字)全保存在cache tag后的内容中
内容由n个字，每个有对应的字地址
![[Pasted image 20250325103506.png|500]]

特点：
	由于行带完整的主存地址，可以将主存中任意一块复制在cache中任意一行中
	所以形式灵活，但为了寻找cache是否存在数据需要比较cache内全部内容
	缺点是高速比较器电路难于设计和实现，因此只适合于小容量 cache 采用。

检索过程：
	在cpu给出主存地址后，cache内所有行的tag与主存地址的块号同时在比较器中比较。（并行提升速度）找出块或行后，再由主存地址的字地址找出块内需要的字
	主存地址：
		块号 s位
		字地址 r位
![[Pasted image 20250325103752.png|500]]

#### 2.直接映射方式（1：N）
但一个主存块只能拷贝到 cache 的一个特定 行位置上去。
cache 的行号 i 和主存的块号 j 有映射关系如下：
$$i = j\mod{m}$$
其中m为cache中的总行数。

![[Pasted image 20250325105534.png|550]]

特点：
	是硬件简单，成本低，地址变换速度快。
	每个主存块只有 一个固定的行位置可存放，容易冲突。
	冲突后就需要交换，交换后可能一段时间后又要交换回来
	会使cache性能下降。
![[Pasted image 20250325112334.png|550]]
检索过程：
	将 s 位的主存块地址分成两部分：低 r 位主存区内块号作为 cache 的行地址，s–r 位区号作为标记(tag)与块数据一起保存在该行。
	首先用 r 位区内块号找到 cache 中的特定一行，然后用地址中的 s–r 位 区号部分与此行的标记在比较器中做比较。
	若命中，用地址中最低的 w 位字地址读取cache行所需求的字

#### 3. 组相联映射方式 (U:N)
将全相联和直接映射组合。
cache 分成 u 组，每组 v 行，所以如果主存中如果分到一组，可以有v个位置存放。
有如下映射关系：
$$\begin{array}{c} m = u\times v\\
组号~ q = j \mod u
\end{array}$$
![[Pasted image 20250325112235.png|500]]

特点：
	组相联映射方式中的每组行数 v 一般取值较小，典型值是 2、4、8、16。这种规模的 v 路比较器容易设计和实现。而块在组中的排放又有一定的灵活性，使冲突减少。为强调比 较器的规模和存放的灵活程度，常称之为 v 路组相联 cache。

![[Pasted image 20250325112304.png|550]]
检索过程：
	CPU 给定一个内存地址访问 cache 时，首先用 d 位区内块号找到 cache 的相应 组，然后将主存地址高 s–d 位区号部分与该组 v 行中的所有标记同时进行比较。哪行的标 记与之相符，哪行即命中。此后再以内存地址的 w 位字地址部分检索此行的具体字


---
### 3.6.3 cache 的替换策略
当一个新的主存块需要拷贝到 cache，而允 许存放此块的行位置都被其他主存块占满时，就要产生替换。（对全相联和组相联 cache需要替换策略，而直接相连只要直接换出即可）
#### 1. 最不经常使用(LFU)算法
LFU 算法认为应将一段时间内被访问次数最少的那行数据换出。
为每行设置计数器，每访问一次，被访行的计数器增 1。
当需要替换时，将计数值最小的行换出。

这种算法将计数周期限定在两次替换之间的间隔时间内，因而不能严格反映近期访问情况。（可能有些行，在历史上有大量的访问，但是近期无访问，无法有效将其替换）

#### 2. 近期最少使用(LRU)算法
LRU 算法将近期内长久未被访问过的行换出。
cache 每命中一次，命中行计数器清零，其他各行计数器增 1。
当需要替换时，比较各特 定行的计数值，将计数值最大的行换出。

保护了刚复制到 cache 中的新数据行，符 合 cache 工作原理，因而使 cache 有较高的命中率。

#### 3. 随机替换 
随机替换策略实际上是不要什么算法，从特定的行位置中随机地选取一行换出即可。
硬件上易于实现，缺点是随意换出的数据很可能马 上又要使用，从而降低命中率和 cache 工作效率。但这个缺陷随着cache容量大小增大而减小。

研究表明，随机替换策略的性能只是稍逊于前两种策略。



---
### 3.6.4 cache 的写操作策略
cache 的内容只是主存部分内容的副本，它应当与主存内容保持一致。
##### 1. 写回法（(write back, copy back）
当CPU 写操作 cache 命中时，只修改cache内容，不立即写入主存。
只有当行被替换回主存时，才写入主存。
这种方法使 cache 真正在 CPU-主存之间读/写两方面都起到 高速缓存作用。

如果 CPU 写 cache 未命中，为了包含欲写字的主存块在 cache 分配一行，将此块整个 复制到 cache 后对其进行修改。主存的写修改操作统一留到换出时再进行。

实现过程中，在cache 行必须配置一个 修改位，以反映此行是否被 CPU 修改过。
- 为0，直接简单丢去。
- 为1，内容写回主存。

缺点：
	存在不一致性的隐患，在并行存储时数据不一致。

##### 2. 全写法(write through)
当写 cache 命中时，cache 与主存同时发生写修改，因而较好地维护了 cache 与主存的内容的一致性。
当写 cache 未命中时，只能直接向主存进行写入。
- WTWA：取主存块到 cache 并为 它分配一个行位置
- WTNWA ：不取主存块到 cache。

缺点：
	cache 对 CPU 向主存的写操作无高速缓冲功能，降低了 cache 的性能。

##### 3. 写一次法(write once)
写一次法是基于写回法并结合全写法的写策略：
写命中与写未命中的处理方法和写回法基本相同，只是第一次写命中时要同时写入主存。

**第一次写命中**
1. **CPU写请求发起**：当目标数据块在缓存中命中，但该块之前从未被写过时（或说是“干净”的状态），CPU发起写操作。
2. **同时写入缓存和主存**：这时不仅更新缓存中的数据，还通过内存总线发起一次存储写周期，将修改内容同步写入主存。
3. **总线嗅探（Snoop）机制启动**：其他处理器的缓存监视到该写操作后，会检查是否持有该数据块的副本，并根据协议（如复制或失效）更新或使之无效，从而确保系统内所有缓存的一致性。
4. **标记状态改变**：该数据块完成第一次写后，状态记录为“已写”，之后不再需要再进行同步写主存操作，进入后续写操作只修改缓存、延迟写回主存的状态（类似于写回法）。


例 3.5  例 3.6 例 3.8