## 1. 初始化
### 1.1 在训练一开始时，loss过高
所以初始化时，应该让所有类别的概率均匀化，所以应该防止logits 中出现过高的项：
1. 将 偏置b 设为0
2. 将 权重w 乘上小数，压缩logits

$\tanh$ 问题：
虽然输入的向量是服从正态分布的，但由于 $\tanh$ 会压缩头部和尾部为1，导致其在梯度下降时，大量梯度在通过这一层后被破坏，降低了梯度下降的速度
