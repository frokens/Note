## 信息
为减少不确定性的量：
$$
I(x) = -\log_2{(p(x))}
$$

> [!理解]
> 一个越不可能的事情发生了，消除的不确定性量越大，信息量越大，而取 $\log_2$ 则是为了与bit 契合，而取 e 为底，称为nats（纳特），以便于计算

## 熵 Entropy: 
平均信息量
$$
H(X) = -\sum_{i=1}^np(x_i)\log{p(x_i)} 
$$

> [!iNFO]
> 是代表一个随机变量的信息量的期望，代表在观察随机变量 X 的多次结果后，我们平均会感到的“意外程度”有多大。

## 互信息mutual infromation
$$
I(X;Y) = \sum_{x\in X}\sum_{y\in Y} p(x,y)\log_2(\frac{p(x,y)}{p(x)p(y)})
$$
以上为离散随机变量情况，对于连续的随机变量将求和改为积分

$$
I(X;Y) = \int\int p(x,y)\log_2\bigg(\frac{p(x,y)}{p(x)p(y)}\bigg)dx dy
$$

用于衡量两个随机变量之间的相关性或共享的信息量，如果两个变量间无关独立，那 $\frac{p(x,y)}{p(x)p(y)}=1$ 取对数就变成了0，代表如果知道了X，那对于发生Y的预测没有任何帮助，在LLM中表述每个token间的关联，和求交叉熵损失和softmax

互信息熵的表达方式（后式被称为条件熵）：
$$
I(X;Y) = H(X) + H(Y) - H(X,Y) = H(X)-H(X|Y) = H(Y) - H(Y|X)
$$

就像求交集的方式一样，通过两个变量减去总体，推导过程大约为将 $\frac{p(x,y)}{p(x)}$看做 $p(x|y)$转为条件变量，去做拆积分

其上限为条件变量相同，下线为两个条件独立

transformer 中的QKA softmax 也是计算各个文字间的关系，通过观察某个token能减少另一个token的不确定性



