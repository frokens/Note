### 引入原因
Loss Function: $L(W) = \sum_{i=1}^N ||w^Tx_i - y_i||$ 
$\hat{W} = (X^TX)^{-1} X^TY$ 
这里$(X^TX)^{-1}X$为X的[[矩阵的左逆，右逆和伪逆|左逆]]，通常 n (样本数) >> p (特征维数) :
然而当 p>n时
容易造成过拟合
解决方法：
1. 加数据，让n>p
2. 特征选择/特征提取 例 [[主成分分析PAC]]
3. 正则化

![[正则化]]

##### 在线性回归模型中，正则化有两种：
1. L1: Lasso, $P(W) = ||W||_1$
2. L2: Ridge 岭回归(权值衰减)，$P(W) = ||W||_2^2 = W^TW$ 

### L2 的解释
#### 以频率的视角
$$\begin{align}J(W) &= \sum_{i=1}^N ||W^Tx_i - y||^2 + \lambda W^TW\\ 
&= (W^TX^T - Y^T)(XW - Y) + \lambda W^TW \\ &= W^T(X^TX +\lambda I)W - 2W^TX^TY + Y^TY  \end{align}$$

所以
$$\begin{align}\hat{W}  &= \arg \min_W{J(W)} \\ \frac{\partial J(W)}{\partial W}  = 2&(X^TX + \lambda I)W -2 X^TY = 0\\ \hat{W} =& (X^TX+\lambda I)^{-1} X^TY\end{align}$$
所以多通过在$X^TX$加入 $\lambda I$ 即让其特征值都大于0， 使得其可逆，同时让逆阵的特征值不会过大。


#### 以贝叶斯视角
![[极大后验估计(MAP)]]

这里我们假设先验概率为[[高斯分布]], $W \sim N(0, \sigma^2_o)$
MAP:
 $$\begin{align}\hat{W} &= \arg{\max_W p(w|y)}\\ &= 
		 \arg{\max{p(y|w)p(w)}} \\ &=  \arg{\max{\log{(\frac{1}{\sqrt{2\pi \sigma}\sqrt{2\pi \sigma_o}})} - \frac{(y-w^Tx)^2}{2}}  + \frac{||W||^2 }{2\sigma_o^2}} \\ &= \arg{\min_W{(W^TX - Y)^2 +\frac{\sigma^2}{\sigma^2_o} ||W||^2} }  \end{align} $$

所以与L2 岭回归的定义相同。
##### 结论
岭回归的 最小二乘法 (Regularized LSE) $\iff$ MAP (参数先验概率和噪声为[[高斯分布]])


### 思考：
>如果X为在特征上满秩(即r(X) = p， 特征不相关), 那么代表$X^TX$ 的逆存在。最小二乘法公式的本质为 $X W = Y$ 即把X作为变换矩阵，而W为p维空间的向量，变为Y向量 在X的列空间(n维)， 如果n= p 且满秩，那求W就异常简单了，直接$W = X^{-1}Y$ （这时我们知道行列空间是双射的，即通过$X^{-1}$ 就可以把Y 完全恢复为W ）。然而不幸的是n (样本数) >> p (特征维数)，但是又由于X在列空间满秩，任意的W一定有一个只与它对应的$\hat{Y}$,  而从Y的视角，Y是由$\hat{Y}$ + 任意一个在 X左零空间的向量 { Null⁡($X^T$)(与Col(X) 正交的子空间}（可能就是噪声？），从而需要一个对应的逆去求解出我们想要的W, 而这个逆就是构造出的左逆（$(X^TX)^{-1}X^T \Rightarrow (X^TX)^{-1}X^TX=I_n$ ）。


>可是现实中可能存在连特征之间都不是线性无关，存在相关的特征，且甚至P>N, 此时多个W对应多个Y, 左逆无效了（$X^TX$ 不存在逆或逆的特征值接近无无限，且左逆是在N>P的情况下构造的）此时可以给$(X^TX)^{-1}$加上  $\lambda I$ 来保证稳定性。甚至直接用伪逆代替左逆
>
>欧氏空间的范数在伪逆的求解中起到度量误差和度量长度的限制作用，伪逆矩阵是在满足几何约束的条件下的最优解，这也是伪逆矩阵的意义所在，例如在最小能量系统中，可使用伪逆求得问题的最优解。



