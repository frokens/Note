## 目录：
矩阵表达：几何意义
概率角度：最小二乘法 $\iff$ noise 为 Gaussian MLE
正则化：
1. L1 $\rightarrow$  Lasso
2. L2 $\rightarrow$ 岭回归 

### 基本变量
$$\begin{array}{c} D = (x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\\X_i \in \mathbb{R} ，y_i \in \mathbb{R}，i = 1,2,\cdots,n \\ X = (x_1,x_2,\cdots,x_n)^T = \begin{pmatrix}
x_{11} & x_{12}  & \cdots   & x_{1m}   \\
x_{21} & x_{22}  & \cdots   & x_{2m}  \\
\vdots & \vdots  & \ddots   & \vdots  \\
x_{n1} & x_{n2}  & \cdots\  & x_{nm}  \\
\end{pmatrix}_{n \times p}\\
Y = (y_1,y_2,\cdots,y_n)^T\end{array}$$
$$f(w) = W^T X$$
这里X和W多一维, $X_0 = 1$ 用于表达常数b
$$L(W) = \sum_{i=1}^n ||W^Tx_i - y_i||^2$$

### 矩阵表达：几何意义
##### 几何解释（一）：
将x矩阵用行空间的想法，对所有点的离预测点的具体的和取最小时的参数
$$\begin{align}L(W) &= \sum_{i=1}^n (W^Tx_i - y_i)^2 \\ &= (W^Tx_1-y_1,\cdots ,W^Tx_n-y_n) \begin{pmatrix}W^Tx_1-y_1\\\vdots \\W^Tx_n-y_n\end{pmatrix} 

\\ &= (W^TX^T - Y^T)(XW - Y) \\ &= W^TX^TXW-2W^TX^TY + Y^TY\end{align}$$

$$\begin{array}{c}\hat{W} = \arg \min(L(W)) \end{array}$$
对W求导以获得L(W)最小值
$$\begin{array}{c}\frac{\partial L(W)}{\partial W} = 2X^TXW - 2X^TY = 0\\ W =  (X^TX)^{-1}X^TY\end{array}$$
##### 几何解释（二）：
将x矩阵用列空间的想法，用n个维度为p的的列向量去线性表示y, 但通常y不能被直接线性表示（在p维的子空间之外，有噪声），所以用y与 x的列空间的上距离最近的向量 去近似y, 而这个向量由x列空间 的线性组合参数 即为W

$$f(w) = W^TX = X^T W$$
由于最近，所以$Y - XW$ 与 $X$  中任意一列都垂直
$$\begin{array}{c}X^T (Y-XW) =0\\ W =  (X^TX)^{-1}X^TY\end{array}$$ 
### 概率视角
推出$L(W) = \sum_{i=1}^n (W^Tx_i - y_i)^2$
