## 目录：
矩阵表达：几何意义

概率角度：
	最小二乘法 $\iff$ MLE(noise 为 [[高斯分布]]) 
正则化：
1. L1 $\rightarrow$  Lasso
2. L2 $\rightarrow$ 岭回归 

![[线性回归-正则化#结论]]

### 基本变量
$$\begin{array}{c} D = (x_1,y_1),(x_2,y_2),\cdots,(x_n,y_n)\\X_i \in \mathbb{R} ，y_i \in \mathbb{R}，i = 1,2,\cdots,n \\ X = (x_1,x_2,\cdots,x_n)^T = \begin{pmatrix}
x_{11} & x_{12}  & \cdots   & x_{1p}   \\
x_{21} & x_{22}  & \cdots   & x_{2p}  \\
\vdots & \vdots  & \ddots   & \vdots  \\
x_{n1} & x_{n2}  & \cdots\  & x_{np}  \\
\end{pmatrix}_{n \times p}\\
Y = (y_1,y_2,\cdots,y_n)^T\end{array}$$
$$f(w) = W^T X$$
这里X和W多一维, $X_0 = 1$ 用于表达常数b
$$L(W) = \sum_{i=1}^n ||W^Tx_i - y_i||^2$$

### 矩阵表达：几何意义
##### 几何解释（一）：
将x矩阵用行空间的想法，对所有点的离预测点的具体的和取最小时的参数
$$\begin{align}L(W) &= \sum_{i=1}^n (W^Tx_i - y_i)^2 \\ &= (W^Tx_1-y_1,\cdots ,W^Tx_n-y_n) \begin{pmatrix}W^Tx_1-y_1\\\vdots \\W^Tx_n-y_n\end{pmatrix} 

\\ &= (W^TX^T - Y^T)(XW - Y) \\ &= W^TX^TXW-2W^TX^TY + Y^TY\end{align}$$

$$\begin{array}{c}\hat{W} = \arg \min(L(W)) \end{array}$$
对W求导以获得L(W)最小值
$$\begin{array}{c}\frac{\partial L(W)}{\partial W} = 2X^TXW - 2X^TY = 0\\ W =  (X^TX)^{-1}X^TY\end{array}$$
##### 几何解释（二）：
将x矩阵用列空间的想法，用n个维度为p的的列向量去线性表示y, 但通常y不能被直接线性表示（在p维的子空间之外，有噪声），所以用y与 x的列空间的上距离最近的向量 去近似y, 而这个向量由x列空间 的线性组合参数 即为W

$$f(w) = W^TX = X^T W$$
由于最近，所以$Y - XW$ 与 $X$  中任意一列都垂直
$$\begin{array}{c}X^T (Y-XW) =0\\ W =  (X^TX)^{-1}X^TY\end{array}$$ 
这里$(X^TX)^{-1}X$也可以用为X的[[矩阵的左逆，右逆和伪逆|左逆]]推出， 通常 n (样本数) >> p (维数)，与[[线性回归-正则化]]有密切关系。 

### 概率视角
推出$L(W) = \sum_{i=1}^n (W^Tx_i - y_i)^2$, 假设噪声服从为$\epsilon \sim N(0,\sigma^2)$
$$\begin{array}{c} y = f(w) + \epsilon \\ y = W^TX + \epsilon \\
y|X,W \sim N(W^TX,\sigma^2)\\MlE: \hfill\\ L(w) = logP(Y|X;W) = log\prod_{x=1}^N P(Y|Y;W) = \sum_{x=1}^N log P(Y|X;W)\\ \quad \quad ~ = \sum_{i=1}^N \log{\frac{1}{\sqrt{2\pi}\sigma}} - \frac{1}{2\sigma^2} (y_i - W^Tx_i)^2 \hfill \\ 
\hat{W} = \arg \max_w L(W)  = \arg \max_W \sum(-\frac{1}{2\sigma^2} (y_i - W^Tx_i)^2) \\ = \arg \min_W \sum_{i=1}^N(W^Tx_i -y_i)^2\end{array}$$

最小正太估计隐藏噪声服从[[高斯分布]]的假设

